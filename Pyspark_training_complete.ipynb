{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('EXAMPLE').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading a CSV file into DataFrame\n",
    "\n",
    "sample_df = spark.read.csv('C:/Users/Lenovo/Desktop/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---------------+\n",
      "|_c0|_c1|_c2|_c3|            _c4|\n",
      "+---+---+---+---+---------------+\n",
      "|5.1|3.5|1.4|0.2|    Iris-setosa|\n",
      "|4.9|  3|1.4|0.2|    Iris-setosa|\n",
      "|4.7|3.2|1.3|0.2|    Iris-setosa|\n",
      "|4.6|3.1|1.5|0.2|    Iris-setosa|\n",
      "|  5|3.6|1.4|0.2|    Iris-setosa|\n",
      "|5.4|3.9|1.7|0.4|    Iris-setosa|\n",
      "|4.6|3.4|1.4|0.3|    Iris-setosa|\n",
      "|  5|3.4|1.5|0.2|    Iris-setosa|\n",
      "|  7|3.2|4.7|1.4|Iris-versicolor|\n",
      "|6.4|3.2|4.5|1.5|Iris-versicolor|\n",
      "|6.9|3.1|4.9|1.5|Iris-versicolor|\n",
      "|5.5|2.3|  4|1.3|Iris-versicolor|\n",
      "|6.5|2.8|4.6|1.5|Iris-versicolor|\n",
      "|5.7|2.8|4.5|1.3|Iris-versicolor|\n",
      "|6.3|3.3|4.7|1.6|Iris-versicolor|\n",
      "|4.9|2.4|3.3|  1|Iris-versicolor|\n",
      "|6.6|2.9|4.6|1.3|Iris-versicolor|\n",
      "|5.2|2.7|3.9|1.4|Iris-versicolor|\n",
      "|  5|  2|3.5|  1|Iris-versicolor|\n",
      "|5.9|  3|4.2|1.5|Iris-versicolor|\n",
      "+---+---+---+---+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count - To count the number of rows in DataFrame\n",
    "sample_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Type - To find the datatype of single column\n",
    "# dtype - To find the datatypes of all columns in a dataframe\n",
    "\n",
    "type(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepal_length: string (nullable = true)\n",
      " |-- sepal_width: string (nullable = true)\n",
      " |-- petal_length: string (nullable = true)\n",
      " |-- petal_width: string (nullable = true)\n",
      " |-- classes: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To assign column names to our dataframe\n",
    "\n",
    "sample_df = sample_df.toDF('sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'classes')\n",
    "\n",
    "sample_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where - to filter the records\n",
    "# alternative to filter\n",
    "# Type 1 df.filter(df.age > 3).collect()\n",
    "# Type 2 df.filter(\"age > 3\").collect()\n",
    "\n",
    "query_1 = sample_df.where(sample_df.petal_width > 1).select(sample_df.sepal_length, sample_df.petal_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|sepal_length|petal_length|\n",
      "+------------+------------+\n",
      "|         5.7|           5|\n",
      "|         5.8|         5.1|\n",
      "|         6.4|         5.3|\n",
      "|         7.7|         6.7|\n",
      "|         7.7|         6.9|\n",
      "|         6.9|         5.7|\n",
      "|         5.6|         4.9|\n",
      "|         7.7|         6.7|\n",
      "|         6.7|         5.7|\n",
      "+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using AND operator in Where condition\n",
    "\n",
    "query_1 = sample_df.where((sample_df.petal_width > 0.2) & (sample_df.classes == \"Iris-setosa\")).select(sample_df.sepal_length, sample_df.petal_length)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|sepal_length|petal_length|\n",
      "+------------+------------+\n",
      "|         5.4|         1.7|\n",
      "|         4.6|         1.4|\n",
      "+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding Mean and Count using Group By and storing to result in newly created Alias column\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "query_2 = sample_df.groupBy(sample_df.classes).agg(F.mean(sample_df.sepal_length).alias(\"sepal_length_mean\"),\n",
    "                               F.count(sample_df.sepal_length).alias(\"sepal_length_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[classes: string, sepal_length_mean: double, sepal_length_count: bigint]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+------------------+\n",
      "|        classes| sepal_length_mean|sepal_length_count|\n",
      "+---------------+------------------+------------------+\n",
      "| Iris-virginica|6.6000000000000005|                14|\n",
      "|    Iris-setosa|            4.9125|                 8|\n",
      "|Iris-versicolor| 5.992307692307692|                13|\n",
      "+---------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join using smaple_df and query_2\n",
    "# Note: Error will be thrown if key column from both tables are same\n",
    "\n",
    "join_df = sample_df.join(query_2, query_2.classes == sample_df.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df = sample_df.join(query_2, \"classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(join_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+-----------+------------+-----------+------------------+------------------+\n",
      "|classes        |sepal_length|sepal_width|petal_length|petal_width|sepal_length_mean |sepal_length_count|\n",
      "+---------------+------------+-----------+------------+-----------+------------------+------------------+\n",
      "|Iris-setosa    |5.1         |3.5        |1.4         |0.2        |4.9125            |8                 |\n",
      "|Iris-setosa    |4.9         |3          |1.4         |0.2        |4.9125            |8                 |\n",
      "|Iris-setosa    |4.7         |3.2        |1.3         |0.2        |4.9125            |8                 |\n",
      "|Iris-setosa    |4.6         |3.1        |1.5         |0.2        |4.9125            |8                 |\n",
      "|Iris-setosa    |5           |3.6        |1.4         |0.2        |4.9125            |8                 |\n",
      "|Iris-setosa    |5.4         |3.9        |1.7         |0.4        |4.9125            |8                 |\n",
      "|Iris-setosa    |4.6         |3.4        |1.4         |0.3        |4.9125            |8                 |\n",
      "|Iris-setosa    |5           |3.4        |1.5         |0.2        |4.9125            |8                 |\n",
      "|Iris-versicolor|7           |3.2        |4.7         |1.4        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|6.4         |3.2        |4.5         |1.5        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|6.9         |3.1        |4.9         |1.5        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|5.5         |2.3        |4           |1.3        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|6.5         |2.8        |4.6         |1.5        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|5.7         |2.8        |4.5         |1.3        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|6.3         |3.3        |4.7         |1.6        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|4.9         |2.4        |3.3         |1          |5.992307692307692 |13                |\n",
      "|Iris-versicolor|6.6         |2.9        |4.6         |1.3        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|5.2         |2.7        |3.9         |1.4        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|5           |2          |3.5         |1          |5.992307692307692 |13                |\n",
      "|Iris-versicolor|5.9         |3          |4.2         |1.5        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|6           |2.2        |4           |1          |5.992307692307692 |13                |\n",
      "|Iris-virginica |5.7         |2.5        |5           |2          |6.6000000000000005|14                |\n",
      "|Iris-virginica |5.8         |2.8        |5.1         |2.4        |6.6000000000000005|14                |\n",
      "|Iris-virginica |6.4         |3.2        |5.3         |2.3        |6.6000000000000005|14                |\n",
      "|Iris-virginica |6.5         |3          |5.5         |1.8        |6.6000000000000005|14                |\n",
      "|Iris-virginica |7.7         |3.8        |6.7         |2.2        |6.6000000000000005|14                |\n",
      "|Iris-virginica |7.7         |2.6        |6.9         |2.3        |6.6000000000000005|14                |\n",
      "|Iris-virginica |6           |2.2        |5           |1.5        |6.6000000000000005|14                |\n",
      "|Iris-virginica |6.9         |3.2        |5.7         |2.3        |6.6000000000000005|14                |\n",
      "|Iris-virginica |5.6         |2.8        |4.9         |2          |6.6000000000000005|14                |\n",
      "|Iris-virginica |7.7         |2.8        |6.7         |2          |6.6000000000000005|14                |\n",
      "|Iris-virginica |6.3         |2.7        |4.9         |1.8        |6.6000000000000005|14                |\n",
      "|Iris-virginica |6.7         |3.3        |5.7         |2.1        |6.6000000000000005|14                |\n",
      "|Iris-virginica |7.2         |3.2        |6           |1.8        |6.6000000000000005|14                |\n",
      "|Iris-virginica |6.2         |2.8        |4.8         |1.8        |6.6000000000000005|14                |\n",
      "+---------------+------------+-----------+------------+-----------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_df.show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[classes: string, sepal_length: string, sepal_width: string, petal_length: string, petal_width: string, sepal_length_mean: double, sepal_length_count: bigint]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+-----------+------------+-----------+------------------+------------------+\n",
      "|classes        |sepal_length|sepal_width|petal_length|petal_width|sepal_length_mean |sepal_length_count|\n",
      "+---------------+------------+-----------+------------+-----------+------------------+------------------+\n",
      "|Iris-setosa    |5.1         |3.5        |1.4         |0.2        |4.9125            |8                 |\n",
      "|Iris-setosa    |4.9         |3          |1.4         |0.2        |4.9125            |8                 |\n",
      "|Iris-setosa    |4.7         |3.2        |1.3         |0.2        |4.9125            |8                 |\n",
      "|Iris-setosa    |4.6         |3.1        |1.5         |0.2        |4.9125            |8                 |\n",
      "|Iris-setosa    |5           |3.6        |1.4         |0.2        |4.9125            |8                 |\n",
      "|Iris-setosa    |5.4         |3.9        |1.7         |0.4        |4.9125            |8                 |\n",
      "|Iris-setosa    |4.6         |3.4        |1.4         |0.3        |4.9125            |8                 |\n",
      "|Iris-setosa    |5           |3.4        |1.5         |0.2        |4.9125            |8                 |\n",
      "|Iris-versicolor|7           |3.2        |4.7         |1.4        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|6.4         |3.2        |4.5         |1.5        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|6.9         |3.1        |4.9         |1.5        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|5.5         |2.3        |4           |1.3        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|6.5         |2.8        |4.6         |1.5        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|5.7         |2.8        |4.5         |1.3        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|6.3         |3.3        |4.7         |1.6        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|4.9         |2.4        |3.3         |1          |5.992307692307692 |13                |\n",
      "|Iris-versicolor|6.6         |2.9        |4.6         |1.3        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|5.2         |2.7        |3.9         |1.4        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|5           |2          |3.5         |1          |5.992307692307692 |13                |\n",
      "|Iris-versicolor|5.9         |3          |4.2         |1.5        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|6           |2.2        |4           |1          |5.992307692307692 |13                |\n",
      "|Iris-virginica |5.7         |2.5        |5           |2          |6.6000000000000005|14                |\n",
      "|Iris-virginica |5.8         |2.8        |5.1         |2.4        |6.6000000000000005|14                |\n",
      "|Iris-virginica |6.4         |3.2        |5.3         |2.3        |6.6000000000000005|14                |\n",
      "|Iris-virginica |6.5         |3          |5.5         |1.8        |6.6000000000000005|14                |\n",
      "|Iris-virginica |7.7         |3.8        |6.7         |2.2        |6.6000000000000005|14                |\n",
      "|Iris-virginica |7.7         |2.6        |6.9         |2.3        |6.6000000000000005|14                |\n",
      "|Iris-virginica |6           |2.2        |5           |1.5        |6.6000000000000005|14                |\n",
      "|Iris-virginica |6.9         |3.2        |5.7         |2.3        |6.6000000000000005|14                |\n",
      "|Iris-virginica |5.6         |2.8        |4.9         |2          |6.6000000000000005|14                |\n",
      "|Iris-virginica |7.7         |2.8        |6.7         |2          |6.6000000000000005|14                |\n",
      "|Iris-virginica |6.3         |2.7        |4.9         |1.8        |6.6000000000000005|14                |\n",
      "|Iris-virginica |6.7         |3.3        |5.7         |2.1        |6.6000000000000005|14                |\n",
      "|Iris-virginica |7.2         |3.2        |6           |1.8        |6.6000000000000005|14                |\n",
      "|Iris-virginica |6.2         |2.8        |4.8         |1.8        |6.6000000000000005|14                |\n",
      "+---------------+------------+-----------+------------+-----------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Could also use 'left_outer'\n",
    "# here join key column is \"classes\" for both dataframes. So mentoin it once inside inverted commas to avoid errors \n",
    "\n",
    "left_join_df = sample_df.join(query_2, \"classes\", how='left') \n",
    "left_join_df.show(150,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_join_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+-----------+------------+-----------+------------------+------------------+\n",
      "|classes        |sepal_length|sepal_width|petal_length|petal_width|sepal_length_mean |sepal_length_count|\n",
      "+---------------+------------+-----------+------------+-----------+------------------+------------------+\n",
      "|Iris-virginica |6.2         |2.8        |4.8         |1.8        |6.6000000000000005|14                |\n",
      "|Iris-virginica |7.2         |3.2        |6           |1.8        |6.6000000000000005|14                |\n",
      "|Iris-virginica |6.7         |3.3        |5.7         |2.1        |6.6000000000000005|14                |\n",
      "|Iris-virginica |6.3         |2.7        |4.9         |1.8        |6.6000000000000005|14                |\n",
      "|Iris-virginica |7.7         |2.8        |6.7         |2          |6.6000000000000005|14                |\n",
      "|Iris-virginica |5.6         |2.8        |4.9         |2          |6.6000000000000005|14                |\n",
      "|Iris-virginica |6.9         |3.2        |5.7         |2.3        |6.6000000000000005|14                |\n",
      "|Iris-virginica |6           |2.2        |5           |1.5        |6.6000000000000005|14                |\n",
      "|Iris-virginica |7.7         |2.6        |6.9         |2.3        |6.6000000000000005|14                |\n",
      "|Iris-virginica |7.7         |3.8        |6.7         |2.2        |6.6000000000000005|14                |\n",
      "|Iris-virginica |6.5         |3          |5.5         |1.8        |6.6000000000000005|14                |\n",
      "|Iris-virginica |6.4         |3.2        |5.3         |2.3        |6.6000000000000005|14                |\n",
      "|Iris-virginica |5.8         |2.8        |5.1         |2.4        |6.6000000000000005|14                |\n",
      "|Iris-virginica |5.7         |2.5        |5           |2          |6.6000000000000005|14                |\n",
      "|Iris-setosa    |5           |3.4        |1.5         |0.2        |4.9125            |8                 |\n",
      "|Iris-setosa    |4.6         |3.4        |1.4         |0.3        |4.9125            |8                 |\n",
      "|Iris-setosa    |5.4         |3.9        |1.7         |0.4        |4.9125            |8                 |\n",
      "|Iris-setosa    |5           |3.6        |1.4         |0.2        |4.9125            |8                 |\n",
      "|Iris-setosa    |4.6         |3.1        |1.5         |0.2        |4.9125            |8                 |\n",
      "|Iris-setosa    |4.7         |3.2        |1.3         |0.2        |4.9125            |8                 |\n",
      "|Iris-setosa    |4.9         |3          |1.4         |0.2        |4.9125            |8                 |\n",
      "|Iris-setosa    |5.1         |3.5        |1.4         |0.2        |4.9125            |8                 |\n",
      "|Iris-versicolor|6           |2.2        |4           |1          |5.992307692307692 |13                |\n",
      "|Iris-versicolor|5.9         |3          |4.2         |1.5        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|5           |2          |3.5         |1          |5.992307692307692 |13                |\n",
      "|Iris-versicolor|5.2         |2.7        |3.9         |1.4        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|6.6         |2.9        |4.6         |1.3        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|4.9         |2.4        |3.3         |1          |5.992307692307692 |13                |\n",
      "|Iris-versicolor|6.3         |3.3        |4.7         |1.6        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|5.7         |2.8        |4.5         |1.3        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|6.5         |2.8        |4.6         |1.5        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|5.5         |2.3        |4           |1.3        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|6.9         |3.1        |4.9         |1.5        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|6.4         |3.2        |4.5         |1.5        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|7           |3.2        |4.7         |1.4        |5.992307692307692 |13                |\n",
      "+---------------+------------+-----------+------------+-----------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Could also use 'left_outer'\n",
    "\n",
    "right_join_df = sample_df.join(query_2, \"classes\",how='right') \n",
    "right_join_df.show(150,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Right join\n",
    "\n",
    "right_join_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+-----------+------------+-----------+------------------+------------------+\n",
      "|classes        |sepal_length|sepal_width|petal_length|petal_width|sepal_length_mean |sepal_length_count|\n",
      "+---------------+------------+-----------+------------+-----------+------------------+------------------+\n",
      "|Iris-virginica |5.7         |2.5        |5           |2          |6.6000000000000005|14                |\n",
      "|Iris-virginica |5.8         |2.8        |5.1         |2.4        |6.6000000000000005|14                |\n",
      "|Iris-virginica |6.4         |3.2        |5.3         |2.3        |6.6000000000000005|14                |\n",
      "|Iris-virginica |6.5         |3          |5.5         |1.8        |6.6000000000000005|14                |\n",
      "|Iris-virginica |7.7         |3.8        |6.7         |2.2        |6.6000000000000005|14                |\n",
      "|Iris-virginica |7.7         |2.6        |6.9         |2.3        |6.6000000000000005|14                |\n",
      "|Iris-virginica |6           |2.2        |5           |1.5        |6.6000000000000005|14                |\n",
      "|Iris-virginica |6.9         |3.2        |5.7         |2.3        |6.6000000000000005|14                |\n",
      "|Iris-virginica |5.6         |2.8        |4.9         |2          |6.6000000000000005|14                |\n",
      "|Iris-virginica |7.7         |2.8        |6.7         |2          |6.6000000000000005|14                |\n",
      "|Iris-virginica |6.3         |2.7        |4.9         |1.8        |6.6000000000000005|14                |\n",
      "|Iris-virginica |6.7         |3.3        |5.7         |2.1        |6.6000000000000005|14                |\n",
      "|Iris-virginica |7.2         |3.2        |6           |1.8        |6.6000000000000005|14                |\n",
      "|Iris-virginica |6.2         |2.8        |4.8         |1.8        |6.6000000000000005|14                |\n",
      "|Iris-setosa    |5.1         |3.5        |1.4         |0.2        |4.9125            |8                 |\n",
      "|Iris-setosa    |4.9         |3          |1.4         |0.2        |4.9125            |8                 |\n",
      "|Iris-setosa    |4.7         |3.2        |1.3         |0.2        |4.9125            |8                 |\n",
      "|Iris-setosa    |4.6         |3.1        |1.5         |0.2        |4.9125            |8                 |\n",
      "|Iris-setosa    |5           |3.6        |1.4         |0.2        |4.9125            |8                 |\n",
      "|Iris-setosa    |5.4         |3.9        |1.7         |0.4        |4.9125            |8                 |\n",
      "|Iris-setosa    |4.6         |3.4        |1.4         |0.3        |4.9125            |8                 |\n",
      "|Iris-setosa    |5           |3.4        |1.5         |0.2        |4.9125            |8                 |\n",
      "|Iris-versicolor|7           |3.2        |4.7         |1.4        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|6.4         |3.2        |4.5         |1.5        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|6.9         |3.1        |4.9         |1.5        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|5.5         |2.3        |4           |1.3        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|6.5         |2.8        |4.6         |1.5        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|5.7         |2.8        |4.5         |1.3        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|6.3         |3.3        |4.7         |1.6        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|4.9         |2.4        |3.3         |1          |5.992307692307692 |13                |\n",
      "|Iris-versicolor|6.6         |2.9        |4.6         |1.3        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|5.2         |2.7        |3.9         |1.4        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|5           |2          |3.5         |1          |5.992307692307692 |13                |\n",
      "|Iris-versicolor|5.9         |3          |4.2         |1.5        |5.992307692307692 |13                |\n",
      "|Iris-versicolor|6           |2.2        |4           |1          |5.992307692307692 |13                |\n",
      "+---------------+------------+-----------+------------+-----------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Could also use 'full_outer'\n",
    "\n",
    "full_outer_join_df = sample_df.join(query_2, \"classes\",how='full') \n",
    "full_outer_join_df.show(150,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_outer_join_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross join\n",
    "\n",
    "#df.crossJoin(df2.select(\"height\")).select(\"age\", \"name\", \"height\").collect()\n",
    "\n",
    "#sample_df.crossJoin(query_2.select(\"classes\")).select(\"sepal_length\",\"sepal_width\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross_join_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+---+---+\n",
      "|petal_length_sepal_length|4.6|5.4|\n",
      "+-------------------------+---+---+\n",
      "|                      1.4|  1|  0|\n",
      "|                      1.7|  0|  1|\n",
      "+-------------------------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crosstab\n",
    "# first colummn(petal_length) will be made distinct and then second column(sepal_length) gives the count of values in that column  \n",
    "\n",
    "query_1.stat.crosstab(\"petal_length\", \"sepal_length\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast # Typcasting\n",
    "# astype\n",
    "\n",
    "typecast_df = sample_df.select(sample_df.petal_length.cast(\"integer\"), sample_df.sepal_length.cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[petal_length: int, sepal_length: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(typecast_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|min(petal_length)|max(petal_length)|\n",
      "+-----------------+-----------------+\n",
      "|                1|                6|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mean, Min, Max\n",
    "\n",
    "typecast_df.select([min(\"petal_length\"), max(\"petal_length\")]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random number generation\n",
    "\n",
    "from pyspark.sql.functions import rand, randn\n",
    "\n",
    "numeric_df = sqlContext.range(0, 10).withColumn('uniform', rand(seed=10)).withColumn('normal', randn(seed=27))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+--------------------+\n",
      "| id|            uniform|              normal|\n",
      "+---+-------------------+--------------------+\n",
      "|  0|0.41371264720975787|  0.5888539012978773|\n",
      "|  1| 0.7311719281896606|  0.8645537008427937|\n",
      "|  2| 0.1982919638208397| 0.06157382353970104|\n",
      "|  3|0.12714181165849525|  0.3623040918178586|\n",
      "|  4| 0.7604318153406678|-0.49575204523675975|\n",
      "|  5|0.12030715258495939|  1.0854146699817222|\n",
      "|  6|0.12131363910425985| -0.5284523629183004|\n",
      "|  7|0.44292918521277047| -0.4798519469521663|\n",
      "|  8| 0.8898784253886249| -0.8820294772950535|\n",
      "|  9|0.03650707717266999| -2.1591956435415334|\n",
      "+---+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numeric_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+--------------------+\n",
      "|summary|                id|            uniform|              normal|\n",
      "+-------+------------------+-------------------+--------------------+\n",
      "|  count|                10|                 10|                  10|\n",
      "|   mean|               4.5| 0.3841685645682706|-0.15825812884638607|\n",
      "| stddev|3.0276503540974917|0.31309395532409323|   0.963345903544872|\n",
      "|    min|                 0|0.03650707717266999| -2.1591956435415334|\n",
      "|    max|                 9| 0.8898784253886249|  1.0854146699817222|\n",
      "+-------+------------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numeric_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+------------------+\n",
      "|      avg(uniform)|       min(uniform)|      max(uniform)|\n",
      "+------------------+-------------------+------------------+\n",
      "|0.3841685645682706|0.03650707717266999|0.8898784253886249|\n",
      "+------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean, min, max\n",
    "\n",
    "numeric_df.select([mean('uniform'), min('uniform'), max('uniform')]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+--------------------+\n",
      "| id|            uniform|              normal|\n",
      "+---+-------------------+--------------------+\n",
      "|  0|0.41371264720975787|  0.5888539012978773|\n",
      "|  1| 0.7311719281896606|  0.8645537008427937|\n",
      "|  2| 0.1982919638208397| 0.06157382353970104|\n",
      "|  3|0.12714181165849525|  0.3623040918178586|\n",
      "|  4| 0.7604318153406678|-0.49575204523675975|\n",
      "|  5|0.12030715258495939|  1.0854146699817222|\n",
      "|  6|0.12131363910425985| -0.5284523629183004|\n",
      "|  7|0.44292918521277047| -0.4798519469521663|\n",
      "|  8| 0.8898784253886249| -0.8820294772950535|\n",
      "|  9|0.03650707717266999| -2.1591956435415334|\n",
      "+---+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numeric_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'uniform', 'normal']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Views\n",
    "#createGlobalTempView\n",
    "#createOrReplaceTempView\n",
    "#createTempView\n",
    "\n",
    "numeric_df.createGlobalTempView(\"numeric_tab\")\n",
    "df2 = spark.sql(\"select * from global_temp.numeric_tab\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|            uniform|\n",
      "+-------+-------------------+\n",
      "|  count|                 10|\n",
      "|   mean| 0.3841685645682706|\n",
      "| stddev|0.31309395532409323|\n",
      "|    min|0.03650707717266999|\n",
      "|    max| 0.8898784253886249|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# describe for one column\n",
    "\n",
    "numeric_df.describe(['uniform']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+--------------------+\n",
      "|summary|                id|            uniform|              normal|\n",
      "+-------+------------------+-------------------+--------------------+\n",
      "|  count|                10|                 10|                  10|\n",
      "|   mean|               4.5| 0.3841685645682706|-0.15825812884638607|\n",
      "| stddev|3.0276503540974917|0.31309395532409323|   0.963345903544872|\n",
      "|    min|                 0|0.03650707717266999| -2.1591956435415334|\n",
      "|    max|                 9| 0.8898784253886249|  1.0854146699817222|\n",
      "+-------+------------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# describe for all columns in a dataframe\n",
    "\n",
    "numeric_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distinct\n",
    "\n",
    "sample_df.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distinct for one particular column\n",
    "\n",
    "sample_df.select('classes').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_df = numeric_df.drop('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(drop_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|            uniform|              normal|\n",
      "+-------------------+--------------------+\n",
      "|0.41371264720975787|  0.5888539012978773|\n",
      "| 0.7311719281896606|  0.8645537008427937|\n",
      "| 0.1982919638208397| 0.06157382353970104|\n",
      "|0.12714181165849525|  0.3623040918178586|\n",
      "| 0.7604318153406678|-0.49575204523675975|\n",
      "|0.12030715258495939|  1.0854146699817222|\n",
      "|0.12131363910425985| -0.5284523629183004|\n",
      "|0.44292918521277047| -0.4798519469521663|\n",
      "| 0.8898784253886249| -0.8820294772950535|\n",
      "|0.03650707717266999| -2.1591956435415334|\n",
      "+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "drop_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "|age|height| name|\n",
      "+---+------+-----+\n",
      "|  5|    80|Alice|\n",
      "|  5|    80|Alice|\n",
      "| 10|    80|Alice|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# here toDF() is used to convert a list of rows into a DataFrame\n",
    "# toDF() is also used to give column names to a DataFrame\n",
    "# Drop duplicate records in a dataframe\n",
    "\n",
    "from pyspark.sql import Row\n",
    "duplicate_df = sc.parallelize([ \\\n",
    "     Row(name='Alice', age=5, height=80), \\\n",
    "     Row(name='Alice', age=5, height=80), \\\n",
    "     Row(name='Alice', age=10, height=80)]).toDF()\n",
    "duplicate_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "|age|height| name|\n",
      "+---+------+-----+\n",
      "|  5|    80|Alice|\n",
      "| 10|    80|Alice|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remove_duplicate_df = duplicate_df.dropDuplicates()\n",
    "remove_duplicate_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "|age|height| name|\n",
      "+---+------+-----+\n",
      "|  5|    80|Alice|\n",
      "|  5|    80|Alice|\n",
      "| 10|    80|Alice|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop null value records in a dataframe\n",
    "\n",
    "duplicate_df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('age', 'bigint'), ('height', 'bigint'), ('name', 'string')]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To find the datatype of all columns in a dataframe\n",
    "\n",
    "duplicate_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Scan ExistingRDD[age#1795L,height#1796L,name#1797]\n"
     ]
    }
   ],
   "source": [
    "duplicate_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', new_age=15),\n",
       " Row(name='Alice', new_age=15),\n",
       " Row(name='Alice', new_age=20)]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select columns\n",
    "duplicate_df.select(duplicate_df.name, (duplicate_df.age + 10).alias('new_age')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=10, height=80, name='Alice'),\n",
       " Row(age=5, height=80, name='Alice'),\n",
       " Row(age=5, height=80, name='Alice')]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sorting columns\n",
    "duplicate_df.sort(duplicate_df.age.desc()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=5, height=80, name='Alice'), Row(age=5, height=80, name='Alice')]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To fetch the first n rows\n",
    "\n",
    "duplicate_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"age\":5,\"height\":80,\"name\":\"Alice\"}'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converts a dataframe to JSON\n",
    "# and also fetch the first row using first()\n",
    "\n",
    "duplicate_df.toJSON().first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>80</td>\n",
       "      <td>Alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>80</td>\n",
       "      <td>Alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>80</td>\n",
       "      <td>Alice</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  height   name\n",
       "0    5      80  Alice\n",
       "1    5      80  Alice\n",
       "2   10      80  Alice"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting a PySpark DataFrame to Pandas DataFrame\n",
    "# Note This method should only be used if the resulting Pandas’s DataFrame is expected to be small, as all the data is loaded into the driver’s memory.\n",
    "\n",
    "duplicate_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------+\n",
      "| name|((age >= 2) AND (age <= 4))|\n",
      "+-----+---------------------------+\n",
      "|Alice|                      false|\n",
      "|Alice|                      false|\n",
      "|Alice|                      false|\n",
      "+-----+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# between with inclusive range\n",
    "\n",
    "duplicate_df.select(duplicate_df.name, duplicate_df.age.between(2, 4)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structfield\n",
    "# getField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------------------+\n",
      "| name|CASE WHEN (age > 3) THEN 1 ELSE 0 END|\n",
      "+-----+-------------------------------------+\n",
      "|Alice|                                    1|\n",
      "|Alice|                                    1|\n",
      "|Alice|                                    1|\n",
      "+-----+-------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Case # When\n",
    "# otherwise\n",
    "#from pyspark.sql import functions as F\n",
    "\n",
    "duplicate_df.select(duplicate_df.name, F.when(duplicate_df.age > 3, 1).otherwise(0)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# substr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row \n",
    "\n",
    "from pyspark.sql import *\n",
    "\n",
    "department1 = Row(id='123456', name='Computer Science')\n",
    "department2 = Row(id='789012', name='Mechanical Engineering')\n",
    "department3 = Row(id='345678', name='Theater and Drama')\n",
    "department4 = Row(id='901234', name='Indoor Recreation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(department1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "Employee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\n",
    "employee1 = Employee('michael', 'armbrust', 'no-reply@berkeley.edu', 100000)\n",
    "employee2 = Employee('xiangrui', 'meng', 'no-reply@stanford.edu', 120000)\n",
    "employee3 = Employee('matei', None, 'no-reply@waterloo.edu', 140000)\n",
    "employee4 = Employee('matei', None, 'no-reply@waterloo.edu', 140000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(employee1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "departmentWithEmployees1 = Row(department=department1, employees=[employee1, employee2])\n",
    "departmentWithEmployees2 = Row(department=department2, employees=[employee3, employee4])\n",
    "departmentWithEmployees3 = Row(department=department3, employees=[employee1, employee4])\n",
    "departmentWithEmployees4 = Row(department=department4, employees=[employee2, employee3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[department: struct<id:string,name:string>, employees: array<struct<firstName:string,lastName:string,email:string,salary:bigint>>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[department: struct<id:string,name:string>, employees: array<struct<firstName:string,lastName:string,email:string,salary:bigint>>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating DataFrame from list of Rows\n",
    "\n",
    "departmentsWithEmployeesSeq1 = [departmentWithEmployees1, departmentWithEmployees2]\n",
    "df1 = spark.createDataFrame(departmentsWithEmployeesSeq1)\n",
    "\n",
    "display(df1)\n",
    "\n",
    "departmentsWithEmployeesSeq2 = [departmentWithEmployees3, departmentWithEmployees4]\n",
    "df2 = spark.createDataFrame(departmentsWithEmployeesSeq2)\n",
    "\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|          department|           employees|\n",
      "+--------------------+--------------------+\n",
      "|[123456,Computer ...|[[michael,armbrus...|\n",
      "|[789012,Mechanica...|[[matei,null,no-r...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|          department|           employees|\n",
      "+--------------------+--------------------+\n",
      "|[345678,Theater a...|[[michael,armbrus...|\n",
      "|[901234,Indoor Re...|[[xiangrui,meng,n...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[department: struct<id:string,name:string>, employees: array<struct<firstName:string,lastName:string,email:string,salary:bigint>>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Union\n",
    "\n",
    "unionDF = df1.unionAll(df2)\n",
    "display(unionDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|          department|           employees|\n",
      "+--------------------+--------------------+\n",
      "|[123456,Computer ...|[[michael,armbrus...|\n",
      "|[789012,Mechanica...|[[matei,null,no-r...|\n",
      "|[345678,Theater a...|[[michael,armbrus...|\n",
      "|[901234,Indoor Re...|[[xiangrui,meng,n...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unionDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|               email|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "|    matei|    null|no-reply@waterloo...|140000|\n",
      "|    matei|    null|no-reply@waterloo...|140000|\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "|    matei|    null|no-reply@waterloo...|140000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "|    matei|    null|no-reply@waterloo...|140000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# assigning column names using selectExpr\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import explode\n",
    "df = unionDF.select(explode(\"employees\").alias(\"e\"))\n",
    "explodeDF = df.selectExpr(\"e.firstName\", \"e.lastName\", \"e.email\", \"e.salary\")\n",
    "\n",
    "explodeDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[firstName: string, lastName: string, email: string, salary: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|               email|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter 1\n",
    "\n",
    "filterDF = explodeDF.filter(explodeDF.firstName == \"xiangrui\").sort(explodeDF.lastName)\n",
    "display(filterDF)\n",
    "filterDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|               email|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter 2\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "filterDF = explodeDF.filter(col(\"firstName\") == \"xiangrui\").sort(explodeDF.lastName)\n",
    "filterDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[firstName: string, lastName: string, email: string, salary: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|               email|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Where - alternative of Filter\n",
    "from pyspark.sql.functions import col, asc\n",
    "\n",
    "whereDF = explodeDF.where((col(\"firstName\") == \"xiangrui\") | (col(\"firstName\") == \"michael\")).sort(asc(\"lastName\"))\n",
    "display(whereDF)\n",
    "whereDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[firstName: string, lastName: string, email: string, salary: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, asc\n",
    "#Or\n",
    "# Use `|` instead of `or`\n",
    "filterDF = explodeDF.filter((col(\"firstName\") == \"xiangrui\") | (col(\"firstName\") == \"michael\")).sort(asc(\"lastName\"))\n",
    "display(filterDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[firstName: string, lastName: string, email: string, salary: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|               email|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "|    matei|      --|no-reply@waterloo...|140000|\n",
      "|    matei|      --|no-reply@waterloo...|140000|\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "|    matei|      --|no-reply@waterloo...|140000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "|    matei|      --|no-reply@waterloo...|140000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replacing null with 'x' values\n",
    "\n",
    "nonNullDF = explodeDF.fillna(\"--\")\n",
    "display(nonNullDF)\n",
    "nonNullDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[firstName: string, lastName: string, email: string, salary: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|               email|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "|    matei|    null|no-reply@waterloo...|140000|\n",
      "|    matei|    null|no-reply@waterloo...|140000|\n",
      "|    matei|    null|no-reply@waterloo...|140000|\n",
      "|    matei|    null|no-reply@waterloo...|140000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fiterter out null value columns\n",
    "\n",
    "filterNonNullDF = explodeDF.filter(col(\"firstName\").isNull() | col(\"lastName\").isNull()).sort(\"email\")\n",
    "display(filterNonNullDF)\n",
    "filterNonNullDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count distinct\n",
    "\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "countDistinctDF = explodeDF.select(\"firstName\", \"lastName\")\\\n",
    "  .groupBy(\"firstName\", \"lastName\")\\\n",
    "  .agg(countDistinct(\"firstName\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------------------------+\n",
      "|firstName|lastName|count(DISTINCT firstName)|\n",
      "+---------+--------+-------------------------+\n",
      "|    matei|    null|                        1|\n",
      "| xiangrui|    meng|                        1|\n",
      "|  michael|armbrust|                        1|\n",
      "+---------+--------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countDistinctDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register the DataFrame as a temp table so that we can query it using SQL\n",
    "explodeDF.registerTempTable(\"df_example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the same query as the DataFrame above and return ``explain``\n",
    "countDistinctDF_sql = spark.sql(\"SELECT firstName, lastName, count(distinct firstName) as distinct_first_names FROM df_example GROUP BY firstName, lastName\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+\n",
      "|firstName|lastName|distinct_first_names|\n",
      "+---------+--------+--------------------+\n",
      "|    matei|    null|                   1|\n",
      "| xiangrui|    meng|                   1|\n",
      "|  michael|armbrust|                   1|\n",
      "+---------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countDistinctDF_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[sum(salary): bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "salarySumDF = explodeDF.agg({\"salary\" : \"sum\"})\n",
    "display(salarySumDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(salary)|\n",
      "+-----------+\n",
      "|    1000000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salarySumDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(explodeDF.salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|            salary|\n",
      "+-------+------------------+\n",
      "|  count|                 8|\n",
      "|   mean|          125000.0|\n",
      "| stddev|17728.105208558365|\n",
      "|    min|            100000|\n",
      "|    max|            140000|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Descriptive statistics of a column\n",
    "\n",
    "explodeDF.describe(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2210a749908>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.clf()\n",
    "pdDF = nonNullDF.toPandas()\n",
    "pdDF.plot(x='firstName', y='salary', kind='bar', rot=45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "random_numbers = sc.parallelize([np.random.rand() for _ in range(1000)], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[711] at parallelize at PythonRDD.scala:540"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.18610008856633709, 0.5846345890554729]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_numbers.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                   e|\n",
      "+--------------------+\n",
      "|[michael,armbrust...|\n",
      "|[xiangrui,meng,no...|\n",
      "|[matei,null,no-re...|\n",
      "|[matei,null,no-re...|\n",
      "|[michael,armbrust...|\n",
      "|[matei,null,no-re...|\n",
      "|[xiangrui,meng,no...|\n",
      "|[matei,null,no-re...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = spark.read.csv('C:/Users/Lenovo/Desktop/details.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String concatenation with delimiter\n",
    "\n",
    "from pyspark.sql.functions import concat, col, lit\n",
    "\n",
    "df_concat = test_df.select(concat(test_df.employee, lit(\" \"), test_df.country))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|concat(employee,  , country)|\n",
      "+----------------------------+\n",
      "|                 shiva india|\n",
      "|             vicky Singapore|\n",
      "|                varun Canada|\n",
      "|             arun  Singapore|\n",
      "|               hemanth India|\n",
      "|                yugesh India|\n",
      "|                   satish US|\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_concat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---+---------+-----------------+\n",
      "|employee|   phone|age|  country|           school|\n",
      "+--------+--------+---+---------+-----------------+\n",
      "|   arun |11111111| 25|Singapore|              smu|\n",
      "| hemanth|22443355| 27|    India|  anna university|\n",
      "|  satish|99884400| 27|       US|              ssn|\n",
      "|   shiva|84386483| 27|    india|              nus|\n",
      "|   varun|99999999| 27|   Canada|              ntu|\n",
      "|   vicky|88888888| 31|Singapore|              nus|\n",
      "|  yugesh|33441122| 27|    India|meenakshi college|\n",
      "+--------+--------+---+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import asc\n",
    "df_asc = test_df.orderBy(asc(\"employee\"))\n",
    "df_asc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "| 27|\n",
      "| 31|\n",
      "| 27|\n",
      "| 25|\n",
      "| 27|\n",
      "| 27|\n",
      "| 27|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting a particular column\n",
    "\n",
    "test_df.select(\"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# denserank\n",
    "\n",
    "#The difference between rank and denseRank is that denseRank leaves no gaps in ranking sequence when there are ties. \n",
    "#That is, if you were ranking a competition using denseRank and had three people tie for second place, \n",
    "#you would say that all three were in second place and that the next person came in third."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|   a|   b|\n",
      "+----+----+\n",
      "|null|null|\n",
      "|   1|null|\n",
      "|null|   2|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Coalesce - returns the first non Null value\n",
    "\n",
    "cDf = spark.createDataFrame([(None, None), (1, None), (None, 2)], (\"a\", \"b\"))\n",
    "cDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|coalesce(a, b)|\n",
      "+--------------+\n",
      "|          null|\n",
      "|             1|\n",
      "|             2|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import coalesce\n",
    "cDf.select(coalesce(cDf[\"a\"], cDf[\"b\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------------+\n",
      "|   a|   b|coalesce(a, 0.0)|\n",
      "+----+----+----------------+\n",
      "|null|null|             0.0|\n",
      "|   1|null|             1.0|\n",
      "|null|   2|             0.0|\n",
      "+----+----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# retuns the first non Null value else the the mentioned literal\n",
    "\n",
    "cDf.select('*', coalesce(cDf[\"a\"], lit(0.0))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenation of string\n",
    "\n",
    "before_df = spark.createDataFrame([('abcd','123')], ['s', 'd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|   s|  d|\n",
      "+----+---+\n",
      "|abcd|123|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "before_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(s='abcd123')]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before_df.select(concat(before_df.s, before_df.d).alias('s')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(s='abcd-123')]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Concatenation of string using a delimiter\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "before_df.select(concat_ws('-', before_df.s, before_df.d).alias('s')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(greatest=4)]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# greatest among the list of columns\n",
    "\n",
    "greatest_df = spark.createDataFrame([(1, 4, 3)], ['a', 'b', 'c'])\n",
    "greatest_df.select(greatest(greatest_df.a, greatest_df.b, greatest_df.c).alias(\"greatest\")).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='SG4205 Information Systems Security'),\n",
       " Row(value='EB5203 Customer Relationship Management'),\n",
       " Row(value=''),\n",
       " Row(value=''),\n",
       " Row(value='KE5207 Computational Intelligence II'),\n",
       " Row(value='EB5202 Web Analytics')]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading a text file into a DataFrame\n",
    "\n",
    "text_df = spark.read.text('C:/Users/Lenovo/Desktop/vig_elective_selection.txt')\n",
    "text_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|SG4205 Informatio...|\n",
      "|EB5203 Customer R...|\n",
      "|                    |\n",
      "|                    |\n",
      "|KE5207 Computatio...|\n",
      "|EB5202 Web Analytics|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col1|col2| pow|\n",
      "+----+----+----+\n",
      "|   1|   2| 1.0|\n",
      "|   2|   3| 8.0|\n",
      "|   3|   3|27.0|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate power values\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import pow, col\n",
    "\n",
    "row = Row(\"col1\", \"col2\")\n",
    "power_df = sc.parallelize([row(1, 2), row(2, 3), row(3, 3)]).toDF()\n",
    "\n",
    "power_df.select(\"*\", pow(col(\"col1\"), col(\"col2\")).alias(\"pow\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(json=Row(a=1))]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading a JSON file into DataFrame\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "data = [(1, '''{\"a\": 1}''')]\n",
    "schema = StructType([StructField(\"a\", IntegerType())])\n",
    "json_df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
    "json_df.select(from_json(json_df.value, schema).alias(\"json\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
